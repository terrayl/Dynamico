A very brief and simple user guide !

Preamble: note that is an evolving document as I will add new stuff when time permits. Note also that only dynamico_monthly.ncl and dynamico_monthly_model.ncl are currently provided, the other code will be there soon ! (see below for the differences between the dynamico codes)

First, please note that this is by no means an extensive user guide ! My main objective here is simply to give a few tips to run efficiently the code 
and get fun results. The first thing to check obviously is to make sure that the user has the ncl library installed (version 6.5 is best). 
IMPORTANT: in addition to the ncl code, the user will also need the two fortran 90 subroutines, distancef.f90 and tewelesf.f90, as well as the corresponding headers distancef.sub and tewelesf.sub. These two routines calculate the two possible distances that currently exist in dynamico, the Euclidean and Teweles-Wobus distances. The user will have to use the WRAPIT wrapper (see https://www.ncl.ucar.edu/Document/Tools/WRAPIT.shtml for the straightforward use of WRAPIT) to generate the two .so files that will be used by the ncl code.

In order to run the ncl codes almost as they are, the user will also need to have climate datasets stored on his server or computer. For example, in order to run dynamico_monthly almost out of the box, the user will need to download monthly sea level pressure (psl) from the 20CR_V3 reanalysis (https://www.psl.noaa.gov/data/gridded/data.20thC_ReanV3.html)  and monthly surface air temperature (sat) from HadCRUT5 (https://crudata.uea.ac.uk/cru/data/temperature/#filfor). In this case, the user will just have to edit the code to change the locations where the data files can be found, as well as changing the directory where the output file is going to be saved (see the Input section of the code). The user may also choose the region where dynamical adjustment is performed. There are a few regions already hard-coded and it is very easy to add new ones.

Depending on the science question, there are three diffferent dynamico ncl codes to choose from: 1. dynamico_monthly.ncl 2. dynamico_daily.ncl 
3. dynamico_monthly_model.ncl. The first two performs dynamical adjustment on observations (or on historical simulations without using constant forcing
pre-industrial simulations) using either monthly or daily data. In these two cases, the circulation analogues are searched in the same dataset (indeed excluding the chosen month or day). 

The third one performs dynamical adjustment using simulated data only and requires both a perturbed and a control simulation. In the standard case, the "perturbed" simulation will be an historical (and/or scenario) simulation with transient forcings and the "control" simulation a simulation with constant pre-industrial forcings. In this case, the circulation analogues of the perturbed simulation will be searched for in the control simulation.

A few tips:

1. Choice of the key parameters: N_a, N_b and niter. Two things here: when the user first runs the code, it is good to have small values for all of them in order to get the results very quickly and make sure that everything works (N_a ~50, N_b ~20, niter ~10). This means that for each month M_i, the code will a) look for the 50 best slp analogues b) will randomly draw 2O out these 50 analogues c) will use these 20 analogues as predictors to best fit slp(M_i) using the Moore-Penrose inverse and derive the fit coefficients d) apply the same fit coefficients to the sat variable to get the dynamical sat component e) repeat the whole sequence 10 times and finally f) average over the 10 iterations to get the best estimate of the dynamical sat component. When everything runs fine with the small values, larger values can then be used. The best choice does depend on the size and location of the slp domain: as a starter, one can use N_a = 100, N_b = 60 and niter = 50. It is always necessary to check the sensitivity of the results to the parameter values (see Deser et al. 2016 for some illustration). With large slp domains (and/or high resolution for the slp field) and large values for the above parameters, the code can take several hours to complete ...

2. Choice of both slp and sat domains: experience shows that the slp domain has to be larger (in both latitude and longitude) than the sat domain but not too much larger. Typically, a 10 and 20 degree delta between the two domain boundaries, for latitude and longitude respectively, seems a good compromise. Again, sensitivity experiments varying the slp domain (assuming that the sat domain is fixed by the science question) are certainly a good thing to do ! 

3. Uncertainties of the dynamical adjustment: in addition to parameter and domain sensitivities alluded above, uncertainty bounds can also be derived a posteriori based on a simple bootstrapping with replacement of the niter estimates. Using different observed datasets for both sat and slp is also necessary to represent observational uncertainty (which can be quite large for slp datasets before 1979 ...)

4. The goodness of the dynamical adjustment: it is actually a difficult question as the truth (the true dynamical component) is usually not known. Some studies have simply tried to maximize the explained variance driven by circulation change and/or compare the dynamical component estimate with other physically-based or statistical insights. A possible methodology would be to implement a carefully designed protocol based on atmospheric nudging experiments that would lead to a "true" estimate of the dynamical component. The objective would then be to compare this "true" estimate with the one from dynamico.

5. Using other variables: the codes on github use slp and sat but it is indeed possible and pretty straightforward to change both variables by changing/adding a few lines in the code.
